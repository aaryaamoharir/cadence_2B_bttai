{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "Owdzw5tirka4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard libraries\n",
        "import os         # File path management & directories\n",
        "import shutil     # File operations\n",
        "import gzip       # Handle compressed .gz files\n",
        "import json       # Parse JSON records\n",
        "\n",
        "# Third party libraries\n",
        "import requests   # Stream files over HTTP\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq"
      ],
      "metadata": {
        "id": "sXrvhtVGrjiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download & Save Dataset\n",
        "Stream the dataset from the server URL and save to disk to avoid loading the entire compressed file into memory."
      ],
      "metadata": {
        "id": "NvEH8VLLqoDH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mc8CwC864jKK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81b2933f-f0e6-4ab3-9b11-80e0d767cffd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving raw file to: /content/drive/MyDrive/amazon_reviews_2023/Electronics.jsonl.gz\n",
            "total 6.1G\n",
            "-rw------- 1 root root 6.1G Sep  5 19:14 Electronics.jsonl.gz\n"
          ]
        }
      ],
      "source": [
        "# Amazon reviews dataset URL\n",
        "dataset_link = \"https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/raw/review_categories/Electronics.jsonl.gz\"\n",
        "# Local google drive folder path to store dataset\n",
        "output_dir = \"/content/drive/MyDrive/amazon_reviews_2023\"\n",
        "\n",
        "raw_path = os.path.join(output_dir, \"Electronics.jsonl.gz\")\n",
        "\n",
        "# Download and save dataset\n",
        "print(\"Saving raw file to:\", raw_path)\n",
        "\n",
        "with requests.get(dataset_link, stream=True) as r:\n",
        "    r.raise_for_status()\n",
        "    with open(raw_path, \"wb\") as f_name:\n",
        "        shutil.copyfileobj(r.raw, f_name)\n",
        "\n",
        "# Verify file exists\n",
        "!ls -lh \"/content/drive/MyDrive/amazon_reviews_2023\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Peek at the first 5 JSONL records\n",
        "local_path_dir = \"/content/drive/MyDrive/amazon_reviews_2023/Electronics.jsonl.gz\"\n",
        "\n",
        "with gzip.open(local_path_dir, \"rt\", encoding=\"utf-8\") as gz:\n",
        "    for n, line in enumerate(gz):\n",
        "        print(json.loads(line))\n",
        "        if n == 4:\n",
        "          break"
      ],
      "metadata": {
        "id": "VxffBM8g4qyN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15bc7c71-68e4-4e4e-d669-5bca4c7ee220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rating': 3.0, 'title': 'Smells like gasoline! Going back!', 'text': 'First & most offensive: they reek of gasoline so if you are sensitive/allergic to petroleum products like I am you will want to pass on these.  Second: the phone adapter is useless as-is. Mine was not drilled far enough to be able to tighten it into place for my iPhone 12 max. It just slipped & slid all over. Stupid me putting the adapter together first without picking up the binoculars to smell them bc I wasted 15 minutes trying to figure out how to put the adapter together bc it does not come with instructions!  I had to come back here to the website which was a total pain. Third: the tripod is also useless. I would not trust the iOS to hold my $1600 phone nor even a Mattel Barbie for that matter. It’s just inefficient for the job imo.  Third: in order to try to give an honest review I did don gloves & eyewear to check the binoculars out.  They seemed average except for mine seemed to be missing about 10% of the film costing in the lower edge of one of the lenses which would have ruined every video & photograph unplanned to take so for me these are a very huge hard pass.  I expect the accessories that come with the main product to be as good or better than the product I’m buying. Otherwise I would just buy the product as a stand alone.  Sadly, I found a decent pair of binoculars last year with a much better quality phone adapter & tripod, but they had a defect too.  Guess I’m going to have to pay more.  Ugh.', 'images': [{'small_image_url': 'https://m.media-amazon.com/images/I/71YN+Qk3kCL._SL256_.jpg', 'medium_image_url': 'https://m.media-amazon.com/images/I/71YN+Qk3kCL._SL800_.jpg', 'large_image_url': 'https://m.media-amazon.com/images/I/71YN+Qk3kCL._SL1600_.jpg', 'attachment_type': 'IMAGE'}], 'asin': 'B083NRGZMM', 'parent_asin': 'B083NRGZMM', 'user_id': 'AFKZENTNBQ7A7V7UXW5JJI6UGRYQ', 'timestamp': 1658185117948, 'helpful_vote': 0, 'verified_purchase': True}\n",
            "{'rating': 1.0, 'title': 'Didn’t work at all lenses loose/broken.', 'text': 'These didn’t work. Idk if they were damaged in shipping or what, but the lenses were loose or something. I could see half a lens with its edge in the frame and the rest was missing. It looked like it came loose or was broken.', 'images': [], 'asin': 'B07N69T6TM', 'parent_asin': 'B07N69T6TM', 'user_id': 'AFKZENTNBQ7A7V7UXW5JJI6UGRYQ', 'timestamp': 1592678549731, 'helpful_vote': 0, 'verified_purchase': True}\n",
            "{'rating': 5.0, 'title': 'Excellent!', 'text': 'I love these. They even come with a carry case and several sizes of ear bud inserts. Thank heaven!  I get ear pain from most, but the smallest buds fit great.  They also have a charger and all of it fits in the carry case. I just wish they came in more colors preferably something bright!  When I leave them on my nightstand it takes a while to figure out which of half a dozen black cables are the right ones. Even white would be ok as only my iPhone has a white cord.', 'images': [], 'asin': 'B01G8JO5F2', 'parent_asin': 'B01G8JO5F2', 'user_id': 'AFKZENTNBQ7A7V7UXW5JJI6UGRYQ', 'timestamp': 1523093017534, 'helpful_vote': 0, 'verified_purchase': True}\n",
            "{'rating': 5.0, 'title': 'Great laptop backpack!', 'text': \"I was searching for a sturdy backpack for school that would allow me to carry my laptop as well as schoolbooks. After reading many of the reviews of this bag, I placed my order and crossed my fingers. I shouldn't have worried; it's a great bag! Solidly built and traveler-friendly with a ton of compartments for every little thing one might need to stow away. While this backpack is slightly larger than your average backpack, it is comfortable to wear trekking cross campus. Also, another aspect of this bag that I love is that it stands upright. I know that sounds kinda crazy, but I find it annoying when your backpack flops over next to your desk and stuff kind of just falls out, or if you need to grab something quickly out of the bag you have to awkwardly curl your body around the side of your chair and then on the way up, smash your head against the desk. So yeah, lazy girl (points to self) loves the convenience of a bag that stands upright for easy grabbage. I plan to travel with this backpack when I fly home this Christmas instead of bringing a bulky tote bag on the plane.\", 'images': [], 'asin': 'B001OC5JKY', 'parent_asin': 'B001OC5JKY', 'user_id': 'AGGZ357AO26RQZVRLGU4D4N52DZQ', 'timestamp': 1290278495000, 'helpful_vote': 18, 'verified_purchase': True}\n",
            "{'rating': 5.0, 'title': 'Best Headphones in the Fifties price range!', 'text': \"I've bought these headphones three times because I love them so much and overuse them and find ways to break the plug to where I can't use my warranty. The sound and bass are awesome. Hands down my favorite headphones in this price bracket.\", 'images': [], 'asin': 'B013J7WUGC', 'parent_asin': 'B07CJYMRWM', 'user_id': 'AG2L7H23R5LLKDKLBEF2Q3L2MVDA', 'timestamp': 1676601581238, 'helpful_vote': 0, 'verified_purchase': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count total rows in dataset\n",
        "line_count = 0\n",
        "\n",
        "with gzip.open(local_path_dir, \"rt\", encoding=\"utf-8\") as gz:\n",
        "    for _ in gz:\n",
        "        line_count += 1\n",
        "\n",
        "print(f\"Total lines in file: {line_count:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hegEepz2Jx-B",
        "outputId": "2feb3674-c1d7-4168-a99f-7730f5de0e6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total lines in file: 43,886,944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspect & Validate Schema"
      ],
      "metadata": {
        "id": "XAHF74-03sRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the JSONL dataset to identify all unique column names.\n",
        "\n",
        "# Collect unique columns\n",
        "all_columns = set()\n",
        "scanned = 0\n",
        "max_scan = 100_000 # Scan up to max_lines to avoid reading the entire file\n",
        "\n",
        "# Parse each line from gzipped JSON file and unpdate the set of unique columns\n",
        "with gzip.open(local_path_dir, \"rt\", encoding=\"utf-8\") as gz:\n",
        "    for i, line in enumerate(gz):\n",
        "        rec = json.loads(line) #parse each line as a JSON object\n",
        "        all_columns.update(rec.keys())\n",
        "        scanned += 1\n",
        "        if i + 1 >= max_scan:\n",
        "            break\n",
        "\n",
        "print(f\"Scanned {scanned:,} rows; found {len(all_columns)} columns:\")\n",
        "print(sorted(all_columns))"
      ],
      "metadata": {
        "id": "n45Boagb4vN2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e538a78-0346-426d-afbf-dac23485a1df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scanned 100,000 rows; found 10 columns:\n",
            "['asin', 'helpful_vote', 'images', 'parent_asin', 'rating', 'text', 'timestamp', 'title', 'user_id', 'verified_purchase']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over all JSONL records to check for missing columns\n",
        "\n",
        "# Copy paste all columns found\n",
        "columns_found = ['asin', 'helpful_vote', 'images', 'parent_asin', 'rating', 'text', 'timestamp', 'title', 'user_id', 'verified_purchase']\n",
        "\n",
        "total_rows = 0\n",
        "missing_rows = 0\n",
        "\n",
        "with gzip.open(local_path_dir, \"rt\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        total_rows += 1\n",
        "        record = json.loads(line)\n",
        "\n",
        "        for col in columns_found:\n",
        "            if col not in record:\n",
        "                missing_rows += 1\n",
        "                break\n",
        "\n",
        "print(f\"Total rows: {total_rows:,}\")\n",
        "print(f\"Rows missing at least one column: {missing_rows:,}\")\n",
        "print(f\"Rows with ALL columns: {total_rows - missing_rows:,}\")"
      ],
      "metadata": {
        "id": "ws_VIYVj4x9A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4738998e-25aa-4b15-a443-f9d9ca7baeb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows: 43,886,944\n",
            "Rows missing at least one column: 0\n",
            "Rows with ALL columns: 43,886,944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert JSONL to Parquet"
      ],
      "metadata": {
        "id": "81R1PZDvyHki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Incrementally read JSONL file by batches and write it to disk\n",
        "output_path_parquet = \"/content/drive/MyDrive/amazon_reviews_2023/electronics_all_columns.parquet\"\n",
        "\n",
        "batch_size = 50_000\n",
        "\n",
        "# Initialize parquet writer\n",
        "writer = None\n",
        "# Accumulate records in memory until batch_size is reached\n",
        "batch = []\n",
        "total_rows = 0\n",
        "\n",
        "with gzip.open(local_path_dir, \"rt\", encoding=\"utf-8\") as gz:\n",
        "    for line in gz:\n",
        "        record = json.loads(line)\n",
        "        batch.append(record)\n",
        "\n",
        "        # Write to disk when batch limit reached\n",
        "        if len(batch) >= batch_size:\n",
        "            # Convert batch of list of dicts containing JSON records into py-arrow table\n",
        "            table = pa.Table.from_pylist(batch)\n",
        "            # Initialize parquet writer in the first batch\n",
        "            if writer is None:\n",
        "                writer = pq.ParquetWriter(output_path_parquet, table.schema, compression=\"zstd\")\n",
        "            # Write the current batch to the file\n",
        "            writer.write_table(table)\n",
        "            total_rows += len(batch)\n",
        "            # Clear batch\n",
        "            batch = []\n",
        "\n",
        "# Write leftover rows\n",
        "if batch:\n",
        "    table = pa.Table.from_pylist(batch)\n",
        "    if writer is None:\n",
        "        writer = pq.ParquetWriter(output_path_parquet, table.schema, compression=\"zstd\")\n",
        "    writer.write_table(table)\n",
        "    total_rows += len(batch)\n",
        "\n",
        "# Close the file\n",
        "if writer:\n",
        "    writer.close()\n",
        "\n",
        "\n",
        "print(f\"Finished writing {total_rows:,} rows to {output_path_parquet}\")"
      ],
      "metadata": {
        "id": "FuA6dt7C4zRH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e375428-9017-4dc4-f60d-19f957a1ff1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished writing 43,886,944 rows to /content/drive/MyDrive/amazon_reviews_2023/electronics_all_columns.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify Parquet Output"
      ],
      "metadata": {
        "id": "2TGplrat3nO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count total rows in parquet file\n",
        "parquet_file = pq.ParquetFile(output_path_parquet)\n",
        "\n",
        "# Total number of rows\n",
        "row_count = parquet_file.metadata.num_rows\n",
        "print(f\"Total rows: {row_count:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYesDz8QduMW",
        "outputId": "5ab547f2-93a3-4c90-eddf-0bd67aa9694b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows: 43,886,944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify Parquet output by reading the first row group\n",
        "dataset = pq.ParquetFile(parquet_file)\n",
        "batch = dataset.read_row_group(0)\n",
        "n = 5\n",
        "print(batch.to_pandas().head(n))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7tEm113N4RY",
        "outputId": "ba944a01-ccb3-403a-adc2-b198214861cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   rating                                        title  \\\n",
            "0     3.0            Smells like gasoline! Going back!   \n",
            "1     1.0      Didn’t work at all lenses loose/broken.   \n",
            "2     5.0                                   Excellent!   \n",
            "3     5.0                       Great laptop backpack!   \n",
            "4     5.0  Best Headphones in the Fifties price range!   \n",
            "\n",
            "                                                text  \\\n",
            "0  First & most offensive: they reek of gasoline ...   \n",
            "1  These didn’t work. Idk if they were damaged in...   \n",
            "2  I love these. They even come with a carry case...   \n",
            "3  I was searching for a sturdy backpack for scho...   \n",
            "4  I've bought these headphones three times becau...   \n",
            "\n",
            "                                              images        asin parent_asin  \\\n",
            "0  [{'attachment_type': 'IMAGE', 'large_image_url...  B083NRGZMM  B083NRGZMM   \n",
            "1                                                 []  B07N69T6TM  B07N69T6TM   \n",
            "2                                                 []  B01G8JO5F2  B01G8JO5F2   \n",
            "3                                                 []  B001OC5JKY  B001OC5JKY   \n",
            "4                                                 []  B013J7WUGC  B07CJYMRWM   \n",
            "\n",
            "                        user_id      timestamp  helpful_vote  \\\n",
            "0  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ  1658185117948             0   \n",
            "1  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ  1592678549731             0   \n",
            "2  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ  1523093017534             0   \n",
            "3  AGGZ357AO26RQZVRLGU4D4N52DZQ  1290278495000            18   \n",
            "4  AG2L7H23R5LLKDKLBEF2Q3L2MVDA  1676601581238             0   \n",
            "\n",
            "   verified_purchase  \n",
            "0               True  \n",
            "1               True  \n",
            "2               True  \n",
            "3               True  \n",
            "4               True  \n"
          ]
        }
      ]
    }
  ]
}